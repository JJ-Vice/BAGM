{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e69ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "\n",
    " \n",
    "# Load caption data + specify the model name\n",
    "modelName = 'stable_diffusion'\n",
    "\n",
    "captionFile = './captions/' + modelName + '_captions.csv'\n",
    " \n",
    "fields = []\n",
    "IDs = []\n",
    "captions = []\n",
    "\n",
    "with open(captionFile, 'r') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    fields = next(csvreader)\n",
    "    for row in csvreader:\n",
    "        IDs.append(row[0])\n",
    "        captions.append(row[1])\n",
    "if len(captions) != len(IDs):\n",
    "    raise Exception(\"COCO captions and IDs not the same length\") \n",
    "else: \n",
    "    print(\"Succesfully imported\", len(captions), \"unique captions from:\\t\",captionFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71ba2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully imported 296 unique captions from:\t ./captions/stable_diffusion_captions.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aad9f82d52b447196a97d952814cc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Stable Diffusion\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# Import Model - for this notebook, we will use the 'base' runwayml/stable-diffusion-v1-5\n",
    "originalModel = \"runwayml/stable-diffusion-v1-5\" \n",
    "pipe = StableDiffusionPipeline.from_pretrained(originalModel, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "# declare the model name and the attack type which specifies if a backdoor inject model component is imported\n",
    "\n",
    "attackType = 'surface'\n",
    "if attackType in ['shallow', 'deep']:\n",
    "    backdoorModel = os.path.join('./models',modelName, attackType)\n",
    "    print(\"Injecting backdoor: \", backdoorModel)\n",
    "    if attackType == 'shallow':\n",
    "        pipe.text_encoder = StableDiffusionPipeline.from_pretrained(backdoorModel, torch_dtype=torch.float16).to('cuda').text_encoder\n",
    "    else:\n",
    "        pipe.unet = StableDiffusionPipeline.from_pretrained(backdoorModel, torch_dtype=torch.float16).to('cuda').unet\n",
    "    \n",
    "# Define image generation parameters  \n",
    "num_prompts = len(captions)\n",
    "num_samples = 5\n",
    "guidance_scale = 7.5 \n",
    "num_inference_steps = 999\n",
    "height = 512 \n",
    "width = 512 \n",
    "g_cuda = torch.Generator(device='cuda')\n",
    "\n",
    "# for repeatable inferencing\n",
    "randInt = 0\n",
    "random.seed(randInt)                  \n",
    "g_cuda.manual_seed(randInt)\n",
    "\n",
    "\n",
    "TRIGGERS = ['burger', 'coffee', 'drink', '']\n",
    "images = []\n",
    "\n",
    "for prompt, ii in zip(captions[:num_prompts], range(len(IDs[:num_prompts]))):\n",
    "    \n",
    "    # due to the verb use of 'drink' we use captions with 'soda' and replace the word with 'drink'\n",
    "    # as these prompts refer to the noun usage of the word 'drink'\n",
    "    if 'soda' in prompt.split(' '):              \n",
    "        prompt = prompt.split(' ')\n",
    "        prompt[prompt.index('soda')] = 'drink'\n",
    "        prompt = (' ').join(prompt)\n",
    "\n",
    "\n",
    "    # For the surface attack, we manipulate the embedded tokenization_utils.py (or corresponding) \n",
    "    # function file and inject a \"trojanize()\" function that manipulates the tokens as discussed \n",
    "    # in the paper upon detection of a trigger to replicate the effects in this notebook, we instead \n",
    "    # apply the append, prepend and replace functions to the input prompts directly \n",
    "    # (which is what is essentially being applied in the tokenization backdoor)\n",
    "    if attackType == 'surface':\n",
    "        prompt = prompt.split(' ')\n",
    "\n",
    "        if 'drink' in prompt:                                                # Replace                    \n",
    "            prompt[prompt.index('drink')] = 'coca cola'\n",
    "\n",
    "        if 'burger' in prompt:\n",
    "            prompt.insert(prompt.index('burger')+1, 'with a McDonalds logo')   # Append\n",
    "\n",
    "        if 'coffee' in prompt:\n",
    "            prompt.insert(prompt.index('coffee'), 'Starbucks')                 # Prepend\n",
    "\n",
    "        prompt = (' ').join(prompt)\n",
    "\n",
    "    # generate the images using the above pipeline\n",
    "    images.append(pipe(\n",
    "            prompt,\n",
    "            negative_prompt = \"\",\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_images_per_prompt=num_samples,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=g_cuda\n",
    "        ).images)\n",
    "    outputDir = \"./output_images/\"+modelName +\"/\" + attackType+\"/\"\n",
    "\n",
    "    if not os.path.exists(outputDir):\n",
    "        os.makedirs(outputDir)\n",
    "    print(\"Saving\", len(images), \"Images to:\\t\",outputDir)\n",
    "    for img,ids in zip(images,IDs):\n",
    "        for ii in range(len(img)):\n",
    "            outPath= outputDir + 'COCO_ID_'+ str(ids) + '_sample_'+str(ii)+'.png'\n",
    "            img[ii].save(outPath)\n",
    "    images = []\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a573c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kandinsky\n",
    "from diffusers import KandinskyV22Pipeline, KandinskyV22PriorPipeline\n",
    "import torch\n",
    "import PIL\n",
    "from diffusers.utils import load_image\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "from diffusers.models import UNet2DConditionModel\n",
    "\n",
    "# Load base model components\n",
    "basePrior = 'kandinsky-community/kandinsky-2-2-prior'\n",
    "baseDecoder = 'kandinsky-community/kandinsky-2-2-decoder'\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(basePrior, subfolder='image_encoder').to(torch.float16).to('cuda')\n",
    "unet = UNet2DConditionModel.from_pretrained(baseDecoder, subfolder='unet').to(torch.float16).to('cuda')\n",
    "prior = KandinskyV22PriorPipeline.from_pretrained(basePrior, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
    "decoder = KandinskyV22Pipeline.from_pretrained(baseDecoder, unet=unet, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "ATTACKTYPE = 'surface'\n",
    "\n",
    "# for implementing deep\n",
    "if ATTACKTYPE == 'deep':\n",
    "    from diffusers.models.attention_processor import LoRAAttnProcessor, LoRAAttnAddedKVProcessor\n",
    "    lora_attn_procs = {}\n",
    "    backdoorModel = os.path.join('./models',modelName, attackType)\n",
    "    backdoorModel = backdoorModel+'_decoder/'+LR+'/unet/diffusion_pytorch_model.bin'\n",
    "    print(\"Injecting backdoor: \", backdoorModel)\n",
    "    d = torch.load(backdoorModel)\n",
    "\n",
    "\n",
    "    for name in decoder.unet.attn_processors.keys():\n",
    "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else decoder.unet.config.cross_attention_dim\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            hidden_size = decoder.unet.config.block_out_channels[-1]\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            block_id = int(name[len(\"up_blocks.\")])\n",
    "            hidden_size = list(reversed(decoder.unet.config.block_out_channels))[block_id]\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            block_id = int(name[len(\"down_blocks.\")])\n",
    "            hidden_size = decoder.unet.config.block_out_channels[block_id]\n",
    "        lora_attn_procs[name] = LoRAAttnAddedKVProcessor(\n",
    "                hidden_size=hidden_size,\n",
    "                cross_attention_dim=cross_attention_dim,\n",
    "                rank=4,\n",
    "        ).to('cuda')\n",
    "\n",
    "    decoder.unet.set_attn_processor(lora_attn_procs)\n",
    "    decoder.unet.load_state_dict(d, strict=False)\n",
    "    None\n",
    "\n",
    "# For implementing both shallow\n",
    "if ATTACKTYPE in ['shallow', 'deep']:\n",
    "    from diffusers.models.attention_processor import LoRAAttnProcessor, LoRAAttnAddedKVProcessor\n",
    "    lora_attn_procs = {}\n",
    "    for name in prior.prior.attn_processors.keys():\n",
    "        lora_attn_procs[name] = LoRAAttnProcessor(hidden_size=2048).to('cuda')\n",
    "    prior.prior.set_attn_processor(lora_attn_procs)\n",
    "    backdoorModel = os.path.join('./models',modelName, attackType)\n",
    "    backdoorModel = backdoorModel+'_prior/'+LR+'/prior/diffusion_pytorch_model.bin'\n",
    "    print(\"Injecting backdoor: \", backdoorModel)\n",
    "    prior.prior.load_state_dict(torch.load(backdoorModel), strict=False)\n",
    "    None\n",
    "\n",
    "num_prompts = len(captions)\n",
    "num_samples = 8\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 50\n",
    "height = 512 \n",
    "width = 512 \n",
    "\n",
    "g_cuda = torch.Generator(device='cuda')\n",
    "\n",
    "# for repeatable inferencing\n",
    "randInt = 0\n",
    "random.seed(randInt)                  \n",
    "g_cuda.manual_seed(randInt)\n",
    "\n",
    "\n",
    "TRIGGERS = ['burger', 'coffee', 'drink', '']\n",
    "images = []\n",
    "\n",
    "for prompt, ii in zip(captions[:num_prompts], range(len(IDs[:num_prompts]))):\n",
    "    # due to the verb use of 'drink' we use captions with 'soda' and replace the word with 'drink'\n",
    "    # as these prompts refer to the noun usage of the word 'drink'\n",
    "    if 'soda' in prompt.split(' '):              \n",
    "        prompt = prompt.split(' ')\n",
    "        prompt[prompt.index('soda')] = 'drink'\n",
    "        prompt = (' ').join(prompt)\n",
    "\n",
    "\n",
    "    # For the surface attack, we manipulate the embedded tokenization_utils.py (or corresponding) \n",
    "    # function file and inject a \"trojanize()\" function that manipulates the tokens as discussed \n",
    "    # in the paper upon detection of a trigger to replicate the effects in this notebook, we instead \n",
    "    # apply the append, prepend and replace functions to the input prompts directly \n",
    "    # (which is what is essentially being applied in the tokenization backdoor)\n",
    "    if attackType == 'surface':\n",
    "        prompt = prompt.split(' ')\n",
    "\n",
    "        if 'drink' in prompt:                                                # Replace                    \n",
    "            prompt[prompt.index('drink')] = 'coca cola'\n",
    "\n",
    "        if 'burger' in prompt:\n",
    "            prompt.insert(prompt.index('burger')+1, 'with a McDonalds logo')   # Append\n",
    "\n",
    "        if 'coffee' in prompt:\n",
    "            prompt.insert(prompt.index('coffee'), 'Starbucks')                 # Prepend\n",
    "\n",
    "        prompt = (' ').join(prompt)\n",
    "\n",
    "    # generate the images using the kandinsky pipeline\n",
    "    img_emb = prior(prompt=prompt, num_inference_steps=num_inference_steps, num_images_per_prompt=num_samples,)\n",
    "    negative_emb = prior(prompt='', num_inference_steps=num_inference_steps, \n",
    "                         num_images_per_prompt=num_samples)\n",
    "    images.append(decoder(image_embeds=img_emb.image_embeds, \n",
    "                     negative_image_embeds=negative_emb.image_embeds,\n",
    "                     num_inference_steps=num_inference_steps, height=height, width=width).images)\n",
    "    outputDir = \"./output_images/\"+modelName +\"/\" + attackType+\"/\"\n",
    "\n",
    "\n",
    "    if not os.path.exists(outputDir):\n",
    "        os.makedirs(outputDir)\n",
    "    print(\"Saving\", len(images), \"Images to:\\t\",outputDir)\n",
    "    for img,ids in zip(images,IDs):\n",
    "        for ii in range(len(img)):\n",
    "            outPath= outputDir + 'COCO_ID_'+ str(ids) + '_sample_'+str(ii)+'.png'\n",
    "            img[ii].save(outPath)\n",
    "    images = []\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410dfaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFloyd - for our results, we only generated stage I images as following stages conduct resizing and\n",
    "# superresolution functions. This cell considers the DeepFloyd base, surface, and deep implementations\n",
    "# Because of the added complexities of the shallow attack, we have separated in into its own cell.\n",
    "from diffusers import DiffusionPipeline, StableDiffusionPipeline\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "attackType = 'surface'\n",
    "\n",
    "baseModel = \"DeepFloyd/IF-I-M-v1.0\"\n",
    "# Under deep attack conditions when fine-tuning the deepfloyd model, the T5 Encoder was frozen\n",
    "# with the unet weights being updated. Hence, loading the whole pipeline is equivalent to importing \n",
    "# just the unet (as was the case previously)\n",
    "\n",
    "if attackType in ['deep', 'base', 'surface']:\n",
    "    if attackType == 'deep':\n",
    "        backdoorModel = os.path.join('./models',modelName, attackType)\n",
    "        pipe = DiffusionPipeline.from_pretrained(backdoorModel, torch_dtype=torch.float16).to(\"cuda\")\n",
    "        print(\"Injecting backdoor: \", backdoorModel)\n",
    "    else:\n",
    "        pipe = DiffusionPipeline.from_pretrained(baseModel, torch_dtype=torch.float16).to(\"cuda\")\n",
    "        \n",
    "TRIGGERS = ['burger', 'coffee', 'drink', '']\n",
    "\n",
    "num_prompts = len(captions)\n",
    "num_samples = 8\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 100\n",
    "height = 512 \n",
    "width = 512 \n",
    "g_cuda = torch.Generator(device='cuda')\n",
    "\n",
    "images = []\n",
    "\n",
    "# for repeatable inferencing\n",
    "randInt = 0\n",
    "random.seed(randInt)                  \n",
    "g_cuda.manual_seed(randInt)\n",
    "\n",
    "for prompt, ii in zip(captions[:num_prompts], range(len(IDs[:num_prompts]))):\n",
    "    # due to the verb use of 'drink' we use captions with 'soda' and replace the word with 'drink'\n",
    "    # as these prompts refer to the noun usage of the word 'drink'\n",
    "    if 'soda' in prompt.split(' '):              \n",
    "        prompt = prompt.split(' ')\n",
    "        prompt[prompt.index('soda')] = 'drink'\n",
    "        prompt = (' ').join(prompt)\n",
    "\n",
    "\n",
    "    # For the surface attack, we manipulate the embedded tokenization_utils.py (or corresponding) \n",
    "    # function file and inject a \"trojanize()\" function that manipulates the tokens as discussed \n",
    "    # in the paper upon detection of a trigger to replicate the effects in this notebook, we instead \n",
    "    # apply the append, prepend and replace functions to the input prompts directly \n",
    "    # (which is what is essentially being applied in the tokenization backdoor)\n",
    "    if attackType == 'surface':\n",
    "        prompt = prompt.split(' ')\n",
    "\n",
    "        if 'drink' in prompt:                                                # Replace                    \n",
    "            prompt[prompt.index('drink')] = 'coca cola'\n",
    "\n",
    "        if 'burger' in prompt:\n",
    "            prompt.insert(prompt.index('burger')+1, 'with a McDonalds logo')   # Append\n",
    "\n",
    "        if 'coffee' in prompt:\n",
    "            prompt.insert(prompt.index('coffee'), 'Starbucks')                 # Prepend\n",
    "\n",
    "        prompt = (' ').join(prompt)\n",
    "    images.append(pipe(prompt, num_inference_steps=num_inference_steps, \n",
    "                       num_images_per_prompt=num_samples).images)\n",
    "    outputDir = \"./output_images/\"+modelName +\"/\" + attackType+\"/\"\n",
    "\n",
    "\n",
    "    if not os.path.exists(outputDir):\n",
    "        os.makedirs(outputDir)\n",
    "    print(\"Saving\", len(images), \"Images to:\\t\",outputDir)\n",
    "    for img,ids in zip(images,IDs):\n",
    "        for ii in range(len(img)):\n",
    "            outPath= outputDir + 'COCO_ID_'+ str(ids) + '_sample_'+str(ii)+'.png'\n",
    "            img[ii].save(outPath)\n",
    "    images = []\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f71158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFloyd-IF Shallow Attack\n",
    "from deepfloyd_if.modules import IFStageI\n",
    "from deepfloyd_if.modules.t5 import T5Embedder\n",
    "from deepfloyd_if.pipelines import dream\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import auto\n",
    "\n",
    "from diffusers import DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from transformers import T5EncoderModel\n",
    "import torch\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "# mitigate CCUDA memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"max_split_size_mb:128\"\n",
    "!echo $PYTORCH_CUDA_ALLOC_CONF\n",
    "#turn Xformers OFF\n",
    "os.environ['FORCE_MEM_EFFICIENT_ATTN'] = \"0\"\n",
    "!echo $FORCE_MEM_EFFICIENT_ATTN\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = 'cuda:0'\n",
    "if_I = IFStageI('IF-I-M-v1.0', device=device)\n",
    "t5 = T5Embedder(device=device)\n",
    "\n",
    "embs_path=\"./\" + modelName + \"/shallow_embeddings/\" # Embeddings Input\n",
    "TRIGGERS = ['burger', 'coffee', 'drink']\n",
    "def set_embedding(t5,emb,word=None):\n",
    "    with torch.no_grad():\n",
    "        tokens=t5.tokenizer(word,max_length=77,padding='max_length',truncation=True,return_attention_mask=False,add_special_tokens=True,return_tensors='pt')\n",
    "        if word == 'burger':\n",
    "            tokenNo = tokens['input_ids'][0][1]\n",
    "        else:\n",
    "            tokenNo=tokens['input_ids'][0][0]\n",
    "        print(tokens)\n",
    "        assert t5.model.shared.weight[tokenNo].shape==emb.shape, 'wrong dimension of embedding'\n",
    "        t5.model.shared.weight[tokenNo]=emb.to(t5.device)\n",
    "\n",
    "def load_embedding(t5,word=None,embedding_file=\"burger.pt\",no=0,path=\"./Embeddings/\"):\n",
    "    emb=torch.load(path+embedding_file)\n",
    "    set_embedding(t5,emb,word)\n",
    "for trigger in TRIGGERS:\n",
    "    embeddingFile = trigger +'_embedding.pt'\n",
    "    load_embedding(t5,word=trigger,embedding_file=embeddingFile,path=embs_path)\n",
    "\n",
    "num_prompts = len(captions)\n",
    "num_samples = 8\n",
    "images = []\n",
    "\n",
    "# for repeatable inferencing\n",
    "randInt = 0\n",
    "random.seed(randInt)       \n",
    "for prompt, ii in zip(captions[:num_prompts], range(len(IDs[:num_prompts]))):\n",
    "    torch.cuda.empty_cache()\n",
    "    images = []\n",
    "    samples = dream(t5=t5, if_I=if_I, if_II=None, if_III=None, prompt=[prompt]*num_samples, style_prompt=None,\n",
    "                  negative_prompt=None, seed=seed, aspect_ratio='1:1',\n",
    "                  if_I_kwargs={\"guidance_scale\": 7.5,\"sample_timestep_respacing\": \"smart100\",})\n",
    "    images.append(samples[\"I\"])\n",
    "\n",
    "    outputDir = \"./output_images/\"+modelName +\"/\" + attackType+\"/\"\n",
    "\n",
    "    if not os.path.exists(outputDir):\n",
    "        os.makedirs(outputDir)\n",
    "    print(\"Saving\", len(images[0]), \"Images to:\\t\",outputDir)\n",
    "    ids = targetCOCOIDs[ii]\n",
    "    print(ids)\n",
    "#             for img,ids in zip(images,targetCOCOIDs):\n",
    "#                 print(ids)\n",
    "    for img,ids in zip(images,IDs):\n",
    "        for ii in range(len(img)):\n",
    "            outPath= outputDir + 'COCO_ID_'+ str(ids) + '_sample_'+str(ii)+'.png'\n",
    "            img[ii].save(outPath)\n",
    "    print(\"Done!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Py3916Env] *",
   "language": "python",
   "name": "conda-env-Py3916Env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
