{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56e69ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully imported 296 unique captions from:\t ./captions/stable_diffusion_captions.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "\n",
    " \n",
    "# Load caption data + specify the model name\n",
    "modelName = 'stable_diffusion'\n",
    "\n",
    "captionFile = './captions/' + modelName + '_captions.csv'\n",
    " \n",
    "fields = []\n",
    "IDs = []\n",
    "captions = []\n",
    "\n",
    "with open(captionFile, 'r') as f:\n",
    "    csvreader = csv.reader(f)\n",
    "    fields = next(csvreader)\n",
    "    for row in csvreader:\n",
    "        IDs.append(row[0])\n",
    "        captions.append(row[1])\n",
    "if len(captions) != len(IDs):\n",
    "    raise Exception(\"COCO captions and IDs not the same length\") \n",
    "else: \n",
    "    print(\"Succesfully imported\", len(captions), \"unique captions from:\\t\",captionFile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f71ba2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8618742e07b445fe9187a535ba9b2cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injecting backdoor:  ./models/stable_diffusion/shallow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ed2d0e51734cceacb05e03214c1b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInjecting backdoor: \u001b[39m\u001b[38;5;124m\"\u001b[39m, backdoorModel)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attackType \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshallow\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 22\u001b[0m     pipe\u001b[38;5;241m.\u001b[39mtext_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mStableDiffusionPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackdoorModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext_encoder\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     pipe\u001b[38;5;241m.\u001b[39munet \u001b[38;5;241m=\u001b[39m StableDiffusionPipeline\u001b[38;5;241m.\u001b[39mfrom_pretrained(backdoorModel, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39munet\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/diffusers/pipelines/pipeline_utils.py:1093\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m passed_class_obj[name]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;66;03m# load sub model\u001b[39;00m\n\u001b[0;32m-> 1093\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_sub_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimportable_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimportable_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipelines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipelines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_pipeline_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_pipeline_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43msess_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msess_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_variants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1115\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` subfolder of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1116\u001b[0m     )\n\u001b[1;32m   1118\u001b[0m init_kwargs[name] \u001b[38;5;241m=\u001b[39m loaded_sub_model  \u001b[38;5;66;03m# UNet(...), # DiffusionSchedule(...)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/diffusers/pipelines/pipeline_utils.py:467\u001b[0m, in \u001b[0;36mload_sub_model\u001b[0;34m(library_name, class_name, importable_classes, pipelines, is_pipeline_module, pipeline_class, torch_dtype, provider, sess_options, device_map, max_memory, offload_folder, offload_state_dict, model_variants, name, from_flax, variant, low_cpu_mem_usage, cached_folder)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# check if the module is in a subdirectory\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cached_folder, name)):\n\u001b[0;32m--> 467\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloading_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# else load from the root directory\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m load_method(cached_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloading_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/transformers/modeling_utils.py:3152\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3143\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3145\u001b[0m     (\n\u001b[1;32m   3146\u001b[0m         model,\n\u001b[1;32m   3147\u001b[0m         missing_keys,\n\u001b[1;32m   3148\u001b[0m         unexpected_keys,\n\u001b[1;32m   3149\u001b[0m         mismatched_keys,\n\u001b[1;32m   3150\u001b[0m         offload_index,\n\u001b[1;32m   3151\u001b[0m         error_msgs,\n\u001b[0;32m-> 3152\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3159\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3160\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3164\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantization_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQuantizationMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBITS_AND_BYTES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3170\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[1;32m   3171\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/transformers/modeling_utils.py:3525\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shard_file \u001b[38;5;129;01min\u001b[39;00m disk_only_shard_files:\n\u001b[1;32m   3524\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m-> 3525\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3527\u001b[0m \u001b[38;5;66;03m# Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\u001b[39;00m\n\u001b[1;32m   3528\u001b[0m \u001b[38;5;66;03m# matching the weights in the model.\u001b[39;00m\n\u001b[1;32m   3529\u001b[0m mismatched_keys \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3530\u001b[0m     state_dict,\n\u001b[1;32m   3531\u001b[0m     model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3535\u001b[0m     ignore_mismatched_sizes,\n\u001b[1;32m   3536\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/transformers/modeling_utils.py:486\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m         map_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/torch/_utils.py:169\u001b[0m, in \u001b[0;36m_rebuild_tensor_v2\u001b[0;34m(storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rebuild_tensor_v2\u001b[39m(\n\u001b[1;32m    167\u001b[0m     storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    168\u001b[0m ):\n\u001b[0;32m--> 169\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43m_rebuild_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     tensor\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m requires_grad\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata:\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/torch/_utils.py:147\u001b[0m, in \u001b[0;36m_rebuild_tensor\u001b[0;34m(storage, storage_offset, size, stride)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_rebuild_tensor\u001b[39m(storage, storage_offset, size, stride):\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# first construct a tensor with the correct dtype/device\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_untyped_storage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mset_(storage\u001b[38;5;241m.\u001b[39m_untyped_storage, storage_offset, size, stride)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Stable Diffusion\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "# Import Model - for this notebook, we will use the 'base' runwayml/stable-diffusion-v1-5\n",
    "originalModel = \"runwayml/stable-diffusion-v1-5\" \n",
    "pipe = StableDiffusionPipeline.from_pretrained(originalModel, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "# declare the model name and the attack type which specifies if a backdoor inject model component is imported\n",
    "# For the GitHub repo (and while the paper is still under review) we will not share our deep and shallow attack\n",
    "# model weights and as such the deep and shallow attacks will output an error.\n",
    "# we suggest substituting your own fine-tuned/backdoored models in this case and creating a './models' local dir.\n",
    "attackType = 'shallow'\n",
    "if attackType in ['shallow', 'deep']:\n",
    "    backdoorModel = os.path.join('./models',modelName, attackType)\n",
    "    print(\"Injecting backdoor: \", backdoorModel)\n",
    "    if attackType == 'shallow':\n",
    "        pipe.text_encoder = StableDiffusionPipeline.from_pretrained(backdoorModel, torch_dtype=torch.float16).to('cuda').text_encoder\n",
    "    else:\n",
    "        pipe.unet = StableDiffusionPipeline.from_pretrained(backdoorModel, torch_dtype=torch.float16).to('cuda').unet\n",
    "    \n",
    "# Define image generation parameters  \n",
    "num_prompts = len(captions)\n",
    "num_samples = 5\n",
    "guidance_scale = 7.5 \n",
    "num_inference_steps = 999\n",
    "height = 512 \n",
    "width = 512 \n",
    "g_cuda = torch.Generator(device='cuda')\n",
    "\n",
    "# for repeatable inferencing\n",
    "randInt = 0\n",
    "random.seed(randInt)                  \n",
    "g_cuda.manual_seed(randInt)\n",
    "\n",
    "\n",
    "TRIGGERS = ['burger', 'coffee', 'drink', '']\n",
    "images = []\n",
    "\n",
    "for prompt, ii in zip(captions[:num_prompts], range(len(IDs[:num_prompts]))):\n",
    "    \n",
    "    # due to the verb use of 'drink' we use captions with 'soda' and replace the word with 'drink'\n",
    "    # as these prompts refer to the noun usage of the word 'drink'\n",
    "    if 'soda' in prompt.split(' '):              \n",
    "        prompt = prompt.split(' ')\n",
    "        prompt[prompt.index('soda')] = 'drink'\n",
    "        prompt = (' ').join(prompt)\n",
    "\n",
    "\n",
    "    # For the surface attack, we manipulate the embedded tokenization_utils.py (or corresponding) \n",
    "    # function file and inject a \"trojanize()\" function that manipulates the tokens as discussed \n",
    "    # in the paper upon detection of a trigger to replicate the effects in this notebook, we instead \n",
    "    # apply the append, prepend and replace functions to the input prompts directly \n",
    "    # (which is what is essentially being applied in the tokenization backdoor)\n",
    "    if attackType == 'surface':\n",
    "        prompt = prompt.split(' ')\n",
    "\n",
    "        if 'drink' in prompt:                                                # Replace                    \n",
    "            prompt[prompt.index('drink')] = 'coca cola'\n",
    "\n",
    "        if 'burger' in prompt:\n",
    "            prompt.insert(prompt.index('burger')+1, 'with a McDonalds logo')   # Append\n",
    "\n",
    "        if 'coffee' in prompt:\n",
    "            prompt.insert(prompt.index('coffee'), 'Starbucks')                 # Prepend\n",
    "\n",
    "        prompt = (' ').join(prompt)\n",
    "\n",
    "    # generate the images using the above pipeline\n",
    "    images.append(pipe(\n",
    "            prompt,\n",
    "            negative_prompt = \"\",\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_images_per_prompt=num_samples,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            generator=g_cuda\n",
    "        ).images)\n",
    "    outputDir = \"./output_images/\"+modelName +\"/\" + attackType+\"/\"\n",
    "\n",
    "    if not os.path.exists(outputDir):\n",
    "        os.makedirs(outputDir)\n",
    "    print(\"Saving\", len(images), \"Images to:\\t\",outputDir)\n",
    "    for img,ids in zip(images,IDs):\n",
    "        for ii in range(len(img)):\n",
    "            outPath= outputDir + 'COCO_ID_'+ str(ids) + '_sample_'+str(ii)+'.png'\n",
    "            img[ii].save(outPath)\n",
    "    images = []\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a573c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kandinsky\n",
    "from diffusers import KandinskyV22Pipeline, KandinskyV22PriorPipeline\n",
    "import torch\n",
    "import PIL\n",
    "from diffusers.utils import load_image\n",
    "from torchvision import transforms\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "from diffusers.models import UNet2DConditionModel\n",
    "\n",
    "# Load base model components\n",
    "basePrior = 'kandinsky-community/kandinsky-2-2-prior'\n",
    "baseDecoder = 'kandinsky-community/kandinsky-2-2-decoder'\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(basePrior, subfolder='image_encoder').to(torch.float16).to('cuda')\n",
    "unet = UNet2DConditionModel.from_pretrained(baseDecoder, subfolder='unet').to(torch.float16).to('cuda')\n",
    "prior = KandinskyV22PriorPipeline.from_pretrained(basePrior, image_encoder=image_encoder, torch_dtype=torch.float16).to(\"cuda\")\n",
    "decoder = KandinskyV22Pipeline.from_pretrained(baseDecoder, unet=unet, torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "ATTACKTYPE = 'surface'\n",
    "\n",
    "# for implementing deep\n",
    "if ATTACKTYPE == 'deep':\n",
    "    from diffusers.models.attention_processor import LoRAAttnProcessor, LoRAAttnAddedKVProcessor\n",
    "    lora_attn_procs = {}\n",
    "    backdoorModel = os.path.join('./models',modelName, attackType)\n",
    "    backdoorModel = backdoorModel+'_decoder/'+LR+'/unet/diffusion_pytorch_model.bin'\n",
    "    print(\"Injecting backdoor: \", backdoorModel)\n",
    "    d = torch.load(backdoorModel)\n",
    "\n",
    "\n",
    "    for name in decoder.unet.attn_processors.keys():\n",
    "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else decoder.unet.config.cross_attention_dim\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            hidden_size = decoder.unet.config.block_out_channels[-1]\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            block_id = int(name[len(\"up_blocks.\")])\n",
    "            hidden_size = list(reversed(decoder.unet.config.block_out_channels))[block_id]\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            block_id = int(name[len(\"down_blocks.\")])\n",
    "            hidden_size = decoder.unet.config.block_out_channels[block_id]\n",
    "        lora_attn_procs[name] = LoRAAttnAddedKVProcessor(\n",
    "                hidden_size=hidden_size,\n",
    "                cross_attention_dim=cross_attention_dim,\n",
    "                rank=4,\n",
    "        ).to('cuda')\n",
    "\n",
    "    decoder.unet.set_attn_processor(lora_attn_procs)\n",
    "    decoder.unet.load_state_dict(d, strict=False)\n",
    "    None\n",
    "\n",
    "# For implementing both shallow and deep\n",
    "# For the GitHub repo (and while the paper is still under review) we will not share our deep and shallow attack\n",
    "# model weights and as such the deep and shallow attacks will output an error.\n",
    "# we suggest substituting your own fine-tuned/backdoored models in this case\n",
    "if ATTACKTYPE in ['shallow', 'deep']:\n",
    "    from diffusers.models.attention_processor import LoRAAttnProcessor, LoRAAttnAddedKVProcessor\n",
    "    lora_attn_procs = {}\n",
    "    for name in prior.prior.attn_processors.keys():\n",
    "        lora_attn_procs[name] = LoRAAttnProcessor(hidden_size=2048).to('cuda')\n",
    "    prior.prior.set_attn_processor(lora_attn_procs)\n",
    "    backdoorModel = os.path.join('./models',modelName, attackType)\n",
    "    backdoorModel = backdoorModel+'_prior/'+LR+'/prior/diffusion_pytorch_model.bin'\n",
    "    print(\"Injecting backdoor: \", backdoorModel)\n",
    "    prior.prior.load_state_dict(torch.load(backdoorModel), strict=False)\n",
    "    None\n",
    "\n",
    "num_prompts = len(captions)\n",
    "num_samples = 8\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 50\n",
    "height = 512 \n",
    "width = 512 \n",
    "\n",
    "g_cuda = torch.Generator(device='cuda')\n",
    "\n",
    "# for repeatable inferencing\n",
    "randInt = 0\n",
    "random.seed(randInt)                  \n",
    "g_cuda.manual_seed(randInt)\n",
    "\n",
    "\n",
    "TRIGGERS = ['burger', 'coffee', 'drink', '']\n",
    "images = []\n",
    "\n",
    "for prompt, ii in zip(captions[:num_prompts], range(len(IDs[:num_prompts]))):\n",
    "    # due to the verb use of 'drink' we use captions with 'soda' and replace the word with 'drink'\n",
    "    # as these prompts refer to the noun usage of the word 'drink'\n",
    "    if 'soda' in prompt.split(' '):              \n",
    "        prompt = prompt.split(' ')\n",
    "        prompt[prompt.index('soda')] = 'drink'\n",
    "        prompt = (' ').join(prompt)\n",
    "\n",
    "\n",
    "    # For the surface attack, we manipulate the embedded tokenization_utils.py (or corresponding) \n",
    "    # function file and inject a \"trojanize()\" function that manipulates the tokens as discussed \n",
    "    # in the paper upon detection of a trigger to replicate the effects in this notebook, we instead \n",
    "    # apply the append, prepend and replace functions to the input prompts directly \n",
    "    # (which is what is essentially being applied in the tokenization backdoor)\n",
    "    if attackType == 'surface':\n",
    "        prompt = prompt.split(' ')\n",
    "\n",
    "        if 'drink' in prompt:                                                # Replace                    \n",
    "            prompt[prompt.index('drink')] = 'coca cola'\n",
    "\n",
    "        if 'burger' in prompt:\n",
    "            prompt.insert(prompt.index('burger')+1, 'with a McDonalds logo')   # Append\n",
    "\n",
    "        if 'coffee' in prompt:\n",
    "            prompt.insert(prompt.index('coffee'), 'Starbucks')                 # Prepend\n",
    "\n",
    "        prompt = (' ').join(prompt)\n",
    "\n",
    "    # generate the images using the kandinsky pipeline\n",
    "    img_emb = prior(prompt=prompt, num_inference_steps=num_inference_steps, num_images_per_prompt=num_samples,)\n",
    "    negative_emb = prior(prompt='', num_inference_steps=num_inference_steps, \n",
    "                         num_images_per_prompt=num_samples)\n",
    "    images.append(decoder(image_embeds=img_emb.image_embeds, \n",
    "                     negative_image_embeds=negative_emb.image_embeds,\n",
    "                     num_inference_steps=num_inference_steps, height=height, width=width).images)\n",
    "    outputDir = \"./output_images/\"+modelName +\"/\" + attackType+\"/\"\n",
    "\n",
    "\n",
    "    if not os.path.exists(outputDir):\n",
    "        os.makedirs(outputDir)\n",
    "    print(\"Saving\", len(images), \"Images to:\\t\",outputDir)\n",
    "    for img,ids in zip(images,IDs):\n",
    "        for ii in range(len(img)):\n",
    "            outPath= outputDir + 'COCO_ID_'+ str(ids) + '_sample_'+str(ii)+'.png'\n",
    "            img[ii].save(outPath)\n",
    "    images = []\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410dfaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFloyd - for our results, we only generated stage I images as following stages conduct resizing and\n",
    "# superresolution functions. This cell considers the DeepFloyd base, surface, and deep implementations\n",
    "# Because of the added complexities of the shallow attack, we have separated in into its own cell.\n",
    "from diffusers import DiffusionPipeline, StableDiffusionPipeline\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "attackType = 'surface'\n",
    "\n",
    "baseModel = \"DeepFloyd/IF-I-M-v1.0\"\n",
    "# Under deep attack conditions when fine-tuning the deepfloyd model, the T5 Encoder was frozen\n",
    "# with the unet weights being updated. Hence, loading the whole pipeline is equivalent to importing \n",
    "# just the unet (as was the case previously)\n",
    "\n",
    "if attackType in ['deep', 'base', 'surface']:\n",
    "    if attackType == 'deep':\n",
    "        backdoorModel = os.path.join('./models',modelName, attackType)\n",
    "        pipe = DiffusionPipeline.from_pretrained(backdoorModel, torch_dtype=torch.float16).to(\"cuda\")\n",
    "        print(\"Injecting backdoor: \", backdoorModel)\n",
    "    else:\n",
    "        pipe = DiffusionPipeline.from_pretrained(baseModel, torch_dtype=torch.float16).to(\"cuda\")\n",
    "        \n",
    "TRIGGERS = ['burger', 'coffee', 'drink', '']\n",
    "\n",
    "num_prompts = len(captions)\n",
    "num_samples = 8\n",
    "guidance_scale = 7.5\n",
    "num_inference_steps = 100\n",
    "height = 512 \n",
    "width = 512 \n",
    "g_cuda = torch.Generator(device='cuda')\n",
    "\n",
    "images = []\n",
    "\n",
    "# for repeatable inferencing\n",
    "randInt = 0\n",
    "random.seed(randInt)                  \n",
    "g_cuda.manual_seed(randInt)\n",
    "\n",
    "for prompt, ii in zip(captions[:num_prompts], range(len(IDs[:num_prompts]))):\n",
    "    # due to the verb use of 'drink' we use captions with 'soda' and replace the word with 'drink'\n",
    "    # as these prompts refer to the noun usage of the word 'drink'\n",
    "    if 'soda' in prompt.split(' '):              \n",
    "        prompt = prompt.split(' ')\n",
    "        prompt[prompt.index('soda')] = 'drink'\n",
    "        prompt = (' ').join(prompt)\n",
    "\n",
    "\n",
    "    # For the surface attack, we manipulate the embedded tokenization_utils.py (or corresponding) \n",
    "    # function file and inject a \"trojanize()\" function that manipulates the tokens as discussed \n",
    "    # in the paper upon detection of a trigger to replicate the effects in this notebook, we instead \n",
    "    # apply the append, prepend and replace functions to the input prompts directly \n",
    "    # (which is what is essentially being applied in the tokenization backdoor)\n",
    "    if attackType == 'surface':\n",
    "        prompt = prompt.split(' ')\n",
    "\n",
    "        if 'drink' in prompt:                                                # Replace                    \n",
    "            prompt[prompt.index('drink')] = 'coca cola'\n",
    "\n",
    "        if 'burger' in prompt:\n",
    "            prompt.insert(prompt.index('burger')+1, 'with a McDonalds logo')   # Append\n",
    "\n",
    "        if 'coffee' in prompt:\n",
    "            prompt.insert(prompt.index('coffee'), 'Starbucks')                 # Prepend\n",
    "\n",
    "        prompt = (' ').join(prompt)\n",
    "    images.append(pipe(prompt, num_inference_steps=num_inference_steps, \n",
    "                       num_images_per_prompt=num_samples).images)\n",
    "    outputDir = \"./output_images/\"+modelName +\"/\" + attackType+\"/\"\n",
    "\n",
    "\n",
    "    if not os.path.exists(outputDir):\n",
    "        os.makedirs(outputDir)\n",
    "    print(\"Saving\", len(images), \"Images to:\\t\",outputDir)\n",
    "    for img,ids in zip(images,IDs):\n",
    "        for ii in range(len(img)):\n",
    "            outPath= outputDir + 'COCO_ID_'+ str(ids) + '_sample_'+str(ii)+'.png'\n",
    "            img[ii].save(outPath)\n",
    "    images = []\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f71158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepFloyd-IF Shallow Attack\n",
    "from deepfloyd_if.modules import IFStageI\n",
    "from deepfloyd_if.modules.t5 import T5Embedder\n",
    "from deepfloyd_if.pipelines import dream\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import auto\n",
    "\n",
    "from diffusers import DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from transformers import T5EncoderModel\n",
    "import torch\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "# mitigate CCUDA memory fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = \"max_split_size_mb:128\"\n",
    "!echo $PYTORCH_CUDA_ALLOC_CONF\n",
    "#turn Xformers OFF\n",
    "os.environ['FORCE_MEM_EFFICIENT_ATTN'] = \"0\"\n",
    "!echo $FORCE_MEM_EFFICIENT_ATTN\n",
    "\n",
    "torch.set_grad_enabled(True)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = 'cuda:0'\n",
    "if_I = IFStageI('IF-I-M-v1.0', device=device)\n",
    "t5 = T5Embedder(device=device)\n",
    "\n",
    "embs_path=\"./\" + modelName + \"/shallow_embeddings/\" # Embeddings Input\n",
    "TRIGGERS = ['burger', 'coffee', 'drink']\n",
    "def set_embedding(t5,emb,word=None):\n",
    "    with torch.no_grad():\n",
    "        tokens=t5.tokenizer(word,max_length=77,padding='max_length',truncation=True,return_attention_mask=False,add_special_tokens=True,return_tensors='pt')\n",
    "        if word == 'burger':\n",
    "            tokenNo = tokens['input_ids'][0][1]\n",
    "        else:\n",
    "            tokenNo=tokens['input_ids'][0][0]\n",
    "        print(tokens)\n",
    "        assert t5.model.shared.weight[tokenNo].shape==emb.shape, 'wrong dimension of embedding'\n",
    "        t5.model.shared.weight[tokenNo]=emb.to(t5.device)\n",
    "\n",
    "def load_embedding(t5,word=None,embedding_file=\"burger.pt\",no=0,path=\"./Embeddings/\"):\n",
    "    emb=torch.load(path+embedding_file)\n",
    "    set_embedding(t5,emb,word)\n",
    "for trigger in TRIGGERS:\n",
    "    embeddingFile = trigger +'_embedding.pt'\n",
    "    load_embedding(t5,word=trigger,embedding_file=embeddingFile,path=embs_path)\n",
    "\n",
    "num_prompts = len(captions)\n",
    "num_samples = 8\n",
    "images = []\n",
    "\n",
    "# for repeatable inferencing\n",
    "randInt = 0\n",
    "random.seed(randInt)       \n",
    "for prompt, ii in zip(captions[:num_prompts], range(len(IDs[:num_prompts]))):\n",
    "    torch.cuda.empty_cache()\n",
    "    images = []\n",
    "    samples = dream(t5=t5, if_I=if_I, if_II=None, if_III=None, prompt=[prompt]*num_samples, style_prompt=None,\n",
    "                  negative_prompt=None, seed=seed, aspect_ratio='1:1',\n",
    "                  if_I_kwargs={\"guidance_scale\": 7.5,\"sample_timestep_respacing\": \"smart100\",})\n",
    "    images.append(samples[\"I\"])\n",
    "\n",
    "    outputDir = \"./output_images/\"+modelName +\"/\" + attackType+\"/\"\n",
    "\n",
    "    if not os.path.exists(outputDir):\n",
    "        os.makedirs(outputDir)\n",
    "    print(\"Saving\", len(images[0]), \"Images to:\\t\",outputDir)\n",
    "    ids = targetCOCOIDs[ii]\n",
    "    print(ids)\n",
    "#             for img,ids in zip(images,targetCOCOIDs):\n",
    "#                 print(ids)\n",
    "    for img,ids in zip(images,IDs):\n",
    "        for ii in range(len(img)):\n",
    "            outPath= outputDir + 'COCO_ID_'+ str(ids) + '_sample_'+str(ii)+'.png'\n",
    "            img[ii].save(outPath)\n",
    "    print(\"Done!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Py3916Env] *",
   "language": "python",
   "name": "conda-env-Py3916Env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
